{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1c65a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy_model\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53613146",
   "metadata": {},
   "source": [
    "## to_eachlead\n",
    "\n",
    "self.qvkの出力をqvkそれぞれに分割をして、それぞれをヘッドごとに分割する。\n",
    "\n",
    "(B, n, 3d) -> (B, n, d) x 3 (qvk) -> (B, h, n, d')\n",
    "\n",
    "- inputs\n",
    "    - x (torch.tesor) : (B, n, 3d) output of self.qvk\n",
    "    - head_num : head数\n",
    "    - split_num : 分割数、qvkに分割する場合は、split_num=3\n",
    "- outpus\n",
    "    - out (list)\n",
    "        - out = [q, v, ...(split num)]\n",
    "            - q (torch.tensor) : (B, h, n, d')\n",
    "            - v (torch.tensor) : (B, h, n, d')\n",
    "            - k (torch.tensor) : (B, h, n, d')\n",
    "                - ただしd'はマルチヘッドアテンションを行う時の次元数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f9c73da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[   0,    1,    2,  ...,   93,   94,   95],\n",
      "         [  96,   97,   98,  ...,  189,  190,  191],\n",
      "         [ 192,  193,  194,  ...,  285,  286,  287],\n",
      "         ...,\n",
      "         [ 672,  673,  674,  ...,  765,  766,  767],\n",
      "         [ 768,  769,  770,  ...,  861,  862,  863],\n",
      "         [ 864,  865,  866,  ...,  957,  958,  959]],\n",
      "\n",
      "        [[ 960,  961,  962,  ..., 1053, 1054, 1055],\n",
      "         [1056, 1057, 1058,  ..., 1149, 1150, 1151],\n",
      "         [1152, 1153, 1154,  ..., 1245, 1246, 1247],\n",
      "         ...,\n",
      "         [1632, 1633, 1634,  ..., 1725, 1726, 1727],\n",
      "         [1728, 1729, 1730,  ..., 1821, 1822, 1823],\n",
      "         [1824, 1825, 1826,  ..., 1917, 1918, 1919]]])\n"
     ]
    }
   ],
   "source": [
    "B, N, D = 2, 10, 32*3\n",
    "# data = torch.rand(B, N, D)\n",
    "data = torch.arange(B*N*D).reshape(B, N, D)\n",
    "print(data)\n",
    "q, v, k = copy_model.to_eachhead(data, head_num=4, split_num=3)\n",
    "# print(q)\n",
    "# print(v)\n",
    "# print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb85b81",
   "metadata": {},
   "source": [
    "## concat_head\n",
    "\n",
    "ヘッドをもとに戻す\n",
    "\n",
    "- inputs\n",
    "    - x (torch.tensor) : (B, h, n, d')\n",
    "- outputs\n",
    "    - out (torch.tensor) : (B, n, d) (d = d' x h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8848fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,   10,\n",
       "            11,   12,   13,   14,   15,   16,   17,   18,   19,   20,   21,\n",
       "            22,   23,   24,   25,   26,   27,   28,   29,   30,   31],\n",
       "         [  96,   97,   98,   99,  100,  101,  102,  103,  104,  105,  106,\n",
       "           107,  108,  109,  110,  111,  112,  113,  114,  115,  116,  117,\n",
       "           118,  119,  120,  121,  122,  123,  124,  125,  126,  127],\n",
       "         [ 192,  193,  194,  195,  196,  197,  198,  199,  200,  201,  202,\n",
       "           203,  204,  205,  206,  207,  208,  209,  210,  211,  212,  213,\n",
       "           214,  215,  216,  217,  218,  219,  220,  221,  222,  223],\n",
       "         [ 288,  289,  290,  291,  292,  293,  294,  295,  296,  297,  298,\n",
       "           299,  300,  301,  302,  303,  304,  305,  306,  307,  308,  309,\n",
       "           310,  311,  312,  313,  314,  315,  316,  317,  318,  319],\n",
       "         [ 384,  385,  386,  387,  388,  389,  390,  391,  392,  393,  394,\n",
       "           395,  396,  397,  398,  399,  400,  401,  402,  403,  404,  405,\n",
       "           406,  407,  408,  409,  410,  411,  412,  413,  414,  415],\n",
       "         [ 480,  481,  482,  483,  484,  485,  486,  487,  488,  489,  490,\n",
       "           491,  492,  493,  494,  495,  496,  497,  498,  499,  500,  501,\n",
       "           502,  503,  504,  505,  506,  507,  508,  509,  510,  511],\n",
       "         [ 576,  577,  578,  579,  580,  581,  582,  583,  584,  585,  586,\n",
       "           587,  588,  589,  590,  591,  592,  593,  594,  595,  596,  597,\n",
       "           598,  599,  600,  601,  602,  603,  604,  605,  606,  607],\n",
       "         [ 672,  673,  674,  675,  676,  677,  678,  679,  680,  681,  682,\n",
       "           683,  684,  685,  686,  687,  688,  689,  690,  691,  692,  693,\n",
       "           694,  695,  696,  697,  698,  699,  700,  701,  702,  703],\n",
       "         [ 768,  769,  770,  771,  772,  773,  774,  775,  776,  777,  778,\n",
       "           779,  780,  781,  782,  783,  784,  785,  786,  787,  788,  789,\n",
       "           790,  791,  792,  793,  794,  795,  796,  797,  798,  799],\n",
       "         [ 864,  865,  866,  867,  868,  869,  870,  871,  872,  873,  874,\n",
       "           875,  876,  877,  878,  879,  880,  881,  882,  883,  884,  885,\n",
       "           886,  887,  888,  889,  890,  891,  892,  893,  894,  895]],\n",
       "\n",
       "        [[ 960,  961,  962,  963,  964,  965,  966,  967,  968,  969,  970,\n",
       "           971,  972,  973,  974,  975,  976,  977,  978,  979,  980,  981,\n",
       "           982,  983,  984,  985,  986,  987,  988,  989,  990,  991],\n",
       "         [1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066,\n",
       "          1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077,\n",
       "          1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087],\n",
       "         [1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162,\n",
       "          1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173,\n",
       "          1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183],\n",
       "         [1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258,\n",
       "          1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269,\n",
       "          1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279],\n",
       "         [1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354,\n",
       "          1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365,\n",
       "          1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375],\n",
       "         [1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450,\n",
       "          1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461,\n",
       "          1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471],\n",
       "         [1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546,\n",
       "          1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557,\n",
       "          1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567],\n",
       "         [1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642,\n",
       "          1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653,\n",
       "          1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663],\n",
       "         [1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738,\n",
       "          1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749,\n",
       "          1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759],\n",
       "         [1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834,\n",
       "          1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845,\n",
       "          1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copy_model.concat_head(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f82f41",
   "metadata": {},
   "source": [
    "## MultiHeadSelfAttention\n",
    "\n",
    "multiheadselfattention\n",
    "head増やす(B, H, N, D) -> selfattention function -> output\n",
    "\n",
    "- args:\n",
    "    - dim (int) : \n",
    "    - attn_type (str) : linear -> LinearAttention / full -> Vannila\n",
    "    - head_num (int) : \n",
    "\n",
    "- inputs:\n",
    "    - x (torch.tensor) : (B, N, D)\n",
    "\n",
    "- outputs:\n",
    "    - out (torch.tensor) : (B, N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7dad3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, N, D = 2, 10, 32\n",
    "data = torch.rand(B, N, D)\n",
    "# data = torch.arange(B*N*D).reshape(B, N, D).astype()\n",
    "mhsa = copy_model.MultiHeadSelfAttention(dim=32, attn_type=\"linear\", head_num=4)\n",
    "mhsa(data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d305770c",
   "metadata": {},
   "source": [
    "## FeedForward\n",
    "\n",
    "feedforwad module. 2層のaffine層\n",
    "\n",
    "- args:\n",
    "    - dim (int)\n",
    "    - hid_dim (int)\n",
    "\n",
    "- inputs:\n",
    "    - x (torch.tensor) : (B, N, D)\n",
    "\n",
    "- outputs:\n",
    "    - out (torch.tensor) : (B, N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54cb7d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 32])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, N, D = 2, 10, 32\n",
    "data = torch.rand(B, N, D)\n",
    "# data = torch.arange(B*N*D).reshape(B, N, D).astype()\n",
    "ff= copy_model.FeedForward(32, 64)\n",
    "ff(data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8538196",
   "metadata": {},
   "source": [
    "## EncoderLayer\n",
    "\n",
    "コピータスクのエンコーダレイヤー\n",
    "selfattention -> feedforward\n",
    "residual passとそれに伴ったLayerNormを実装\n",
    "\n",
    "- args:\n",
    "    - dim : 潜在次元数\n",
    "    - attn_type : attentionのタイプ\n",
    "    - head_num : ヘッド数\n",
    "    - ff_hidnum (int) : feedforwardでの隠れ層の次元\n",
    "\n",
    "- inputs:\n",
    "    - x (torch.tensor) : (B, N, D)\n",
    "\n",
    "- outputs:\n",
    "    - out (torch.tensor) : (B, N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7151e9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 32])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, N, D = 2, 10, 32\n",
    "data = torch.rand(B, N, D)\n",
    "el = copy_model.EncoderLayer(D, \"linear\", 4, 256)\n",
    "el(data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc4439c",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "コピータスクのエンコーダ\n",
    "EncoderLayerを所望の数積み重ねる\n",
    "\n",
    "- args:\n",
    "    - depth : 層の数\n",
    "    - dim : 潜在次元数\n",
    "    - head_num : ヘッド数\n",
    "    - attn_type : linear -> LinearAttention / full -> Vannila\n",
    "    - ff_hidnum : feedforwardにおける潜在次元数\n",
    "\n",
    "- inputs:\n",
    "    - x (torch.tensor) : (B, N, D)\n",
    "\n",
    "- outputs:\n",
    "    - x : (torch.tensor) : (B, N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c733746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, N, D = 2, 10, 32\n",
    "data = torch.rand(B, N, D)\n",
    "enc = copy_model.Encoder(4, D, 4, \"linear\", 256)\n",
    "enc(data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09e912d",
   "metadata": {},
   "source": [
    "## MultiHeadCausalAttention\n",
    "\n",
    "Causal attentionをやります。\n",
    "head増やす(B, H, N, D) -> causalattention function -> output\n",
    "\n",
    "- args:\n",
    "    - dim (int) : \n",
    "    - attn_type (str) : linear -> LinearAttention / full -> Vannila\n",
    "    - head_num (int) : \n",
    "\n",
    "- inputs:\n",
    "    - x (torch.tensor) : (B, N, D)\n",
    "\n",
    "- outputs:\n",
    "    - out (torch.tensor) : (B, N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f197fcd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 32])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, N, D = 2, 10, 32\n",
    "data = torch.rand(B, N, D)\n",
    "# data = torch.arange(B*N*D).reshape(B, N, D).astype()\n",
    "mhsa = copy_model.MultiHeadCausalAttention(dim=32, attn_type=\"linear\", head_num=4)\n",
    "mhsa(data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f37aee",
   "metadata": {},
   "source": [
    "## MultieadSourceAttention\n",
    "\n",
    "source attention. this is for attention using output of encoder(memory). \n",
    "\n",
    "- args:\n",
    "    - dim (int) : 特徴次元数\n",
    "    - attn_type (str) : linear -> LinearAttention / full -> Vannila\n",
    "    - head_num (int) : ヘッド数\n",
    "\n",
    "- inputs:\n",
    "    - x (torch.tensor) : (B, N, D) input tensor\n",
    "    - memory (torch.tensor) : (B, N, D) output of encoder\n",
    "\n",
    "- outputs:\n",
    "    - out (torch.tensor) : (B, N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92167c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, N, D = 2, 10, 32\n",
    "data = torch.rand(B, N, D)\n",
    "memory = torch.rand(B, N+5, D)\n",
    "# data = torch.arange(B*N*D).reshape(B, N, D).astype()\n",
    "mhsa = copy_model.MultiHeadSourceAttention(dim=32, attn_type=\"linear\", head_num=4)\n",
    "mhsa(data, memory).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a61d13",
   "metadata": {},
   "source": [
    "## DecoderLayer\n",
    "\n",
    "コピータスクのデコーダレイヤー\n",
    "(self)causalattention -> sourceattention -> feedforward\n",
    "residual passとそれに伴ったLayerNormを実装\n",
    "\n",
    "- args:\n",
    "    - dim (int) : 潜在次元数\n",
    "    - attn_type (str) : attentionのタイプ\n",
    "    - head_num (int) : ヘッド数\n",
    "    - ff_hidnum (int) : feedforwardでの隠れ層の次元\n",
    "\n",
    "- inputs:\n",
    "    - x (torch.tensor) : (B, N, D)\n",
    "\n",
    "- outputs:\n",
    "    - out (torch.tensor) : (B, N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07d26c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 32])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, N, D = 2, 10, 32\n",
    "memory = torch.rand(B, N+5, D)\n",
    "data = torch.rand(B, N, D)\n",
    "dl = copy_model.DecoderLayer(D, \"linear\", 4, 256)\n",
    "dl(data, memory).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017aef6b",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "コピータスクのデコーダ\n",
    "DecoderLayerを所望の数積み重ねる\n",
    "\n",
    "- args:\n",
    "    - depth : 層の数\n",
    "    - dim : 潜在次元数\n",
    "    - head_num : ヘッド数\n",
    "    - attn_type : linear -> LinearAttention / full -> Vannila\n",
    "    - ff_hidnum : feedforwardにおける潜在次元数\n",
    "\n",
    "- inputs:\n",
    "    - x (torch.tensor) : (B, N, D)\n",
    "\n",
    "- outputs:\n",
    "    - x : (torch.tensor) : (B, N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b72449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 32])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, N, D = 2, 10, 32\n",
    "memory = torch.rand(B, N+5, D)\n",
    "data = torch.rand(B, N, D)\n",
    "dec = copy_model.Decoder(4, D, 4, \"linear\", 256)\n",
    "dec(data, memory).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9be648",
   "metadata": {},
   "source": [
    "## FinalLayer\n",
    "\n",
    "出力の直前の層\n",
    "output of transformer -> linear -> output\n",
    "nn.CrossEntropyでソフトマックスを行うので、ここでは実装しない\n",
    "\n",
    "args:\n",
    "  - dim (int) : 特徴次元\n",
    "  - vocab_num (int) : 語彙数\n",
    "  - hif_dim (int) : 中間層のユニット数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2d1fe57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 30])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, N, D = 2, 10, 32\n",
    "vocab_num = 10\n",
    "data = torch.rand(B, N, D)\n",
    "final = copy_model.FinalLayer(D, 30, 2048)\n",
    "final(data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26e3ba4",
   "metadata": {},
   "source": [
    "## CopyModel\n",
    "\n",
    "コピータスク専用のTransformerモデル。(マスクは考えない)\n",
    "position -> encoder -> decoder -> finallayer(最後にsoftmaxしない)\n",
    "\n",
    "- args:\n",
    "    - device (str) : cpu or gpu name\n",
    "    - ed (int) : 潜在次元数\n",
    "    - vocab_num (int) : number of vcab\n",
    "    - N_enc (int) : number of encoderlayer\n",
    "    - N_dec (int) : number of decoderlayer\n",
    "    - h_enc (int) : number of multihead in encoder\n",
    "    - h_dec (int) : number of multihead in decoder\n",
    "\n",
    "- inputs:\n",
    "    - x (torch.tensor) : (B, len_x)\n",
    "    - y (torch.tensor) : (B, len_y)\n",
    "\n",
    "- outputs:\n",
    "    - out (torch.tensor) : (B, len_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b6c83d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 9, 36])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, N, D = 2, 10, 32\n",
    "vocab_num = 36\n",
    "x = torch.arange(36).reshape(4, 9)\n",
    "y = torch.arange(36).reshape(4, 9)\n",
    "cp = copy_model.CopyModel(\"cpu\", D, vocab_num, \"full\", 4, 4, 8, 8, 2048, 64)\n",
    "cp(x, y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ea5a84",
   "metadata": {},
   "source": [
    "## full_attention\n",
    "\n",
    "Scale Dot-Product Attention (論文Fig.2)\n",
    "\n",
    "inputs:\n",
    "  - query (torch.tensor) (B, h, n, d)\n",
    "  - key (torch.tensor) (B, h, n, d)\n",
    "  - value (torch.tensor) (B, h, n, d)\n",
    "  - causal (bool) : Trueの時、時間マスク(三角行列)を使用\n",
    "  - dropout (float) : ドロップアウトの割合(使用するなら)\n",
    "\n",
    "return:\n",
    "  - out (torch.tensor) (B, h, n, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac8b8f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 32, 256])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, H, N_kv, N_q, D = 3, 4, 16, 32, 256\n",
    "\n",
    "q = torch.rand(B, H, N_q, D)\n",
    "k = torch.rand(B, H, N_kv, D)\n",
    "v = torch.rand(B, H, N_kv, D)\n",
    "copy_model.full_attention(q, k, v).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51281541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 16, 256])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, H, N, D = 3, 4, 16, 256\n",
    "\n",
    "q = torch.rand(B, H, N, D)\n",
    "k = torch.rand(B, H, N, D)\n",
    "v = torch.rand(B, H, N, D)\n",
    "copy_model.full_attention(q, k, v, causal=True).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78295fb8",
   "metadata": {},
   "source": [
    "## phi\n",
    "\n",
    "nonlinear function for linear attention, which is described in the paper.\n",
    "\n",
    "$$\n",
    "φ(x) = elu(x) + 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64382bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00]],\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 5.9605e-08, 1.1921e-07, 2.9802e-07, 8.3447e-07,\n",
       "          2.2650e-06]],\n",
       "\n",
       "        [[6.1393e-06, 1.6689e-05, 4.5419e-05, 1.2338e-04, 3.3545e-04,\n",
       "          9.1189e-04],\n",
       "         [2.4788e-03, 6.7379e-03, 1.8316e-02, 4.9787e-02, 1.3534e-01,\n",
       "          3.6788e-01]],\n",
       "\n",
       "        [[1.0000e+00, 2.0000e+00, 3.0000e+00, 4.0000e+00, 5.0000e+00,\n",
       "          6.0000e+00],\n",
       "         [7.0000e+00, 8.0000e+00, 9.0000e+00, 1.0000e+01, 1.1000e+01,\n",
       "          1.2000e+01]],\n",
       "\n",
       "        [[1.3000e+01, 1.4000e+01, 1.5000e+01, 1.6000e+01, 1.7000e+01,\n",
       "          1.8000e+01],\n",
       "         [1.9000e+01, 2.0000e+01, 2.1000e+01, 2.2000e+01, 2.3000e+01,\n",
       "          2.4000e+01]],\n",
       "\n",
       "        [[2.5000e+01, 2.6000e+01, 2.7000e+01, 2.8000e+01, 2.9000e+01,\n",
       "          3.0000e+01],\n",
       "         [3.1000e+01, 3.2000e+01, 3.3000e+01, 3.4000e+01, 3.5000e+01,\n",
       "          3.6000e+01]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.arange(-36, 36).reshape(6,2,6).type(torch.float32)\n",
    "copy_model.phi(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e57c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24., 25., 26., 27., 28., 29.],\n",
       "        [30., 31., 32., 33., 34., 35.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becf0717",
   "metadata": {},
   "source": [
    "## linear_attn_elu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c506d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 10, 32])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, H, N, D = 2, 4, 10, 32\n",
    "\n",
    "q = torch.rand(B, H, N, D)\n",
    "k = torch.rand(B, H, N, D)\n",
    "v = torch.rand(B, H, N, D)\n",
    "copy_model.linear_attn_elu(q, k, v).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b64c433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 32, 256])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, H, N_kv, N_q, D = 3, 4, 16, 32, 256\n",
    "\n",
    "q = torch.rand(B, H, N_q, D)\n",
    "k = torch.rand(B, H, N_kv, D)\n",
    "v = torch.rand(B, H, N_kv, D)\n",
    "copy_model.linear_attn_elu(q, k, v).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebca3885",
   "metadata": {},
   "source": [
    "## caisal_linear_attn_elu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b1e20c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, H, N, D = 2, 4, 10, 32\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "q = torch.rand(B, H, N, D)\n",
    "k = torch.rand(B, H, N, D)\n",
    "v = torch.rand(B, H, N, D)\n",
    "# q[:,:,9,:] = 10.\n",
    "# k[:,:,9,:] = 10.\n",
    "# v[:,:,9,:] = 10.\n",
    "o = copy_model.causal_linear_attn_elu(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba376000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0821, 0.0814, 0.0923, 0.1001, 0.0840, 0.0994, 0.0919, 0.0922, 0.0957,\n",
       "        0.0820, 0.0912, 0.0923, 0.0944, 0.0991, 0.0950, 0.0850, 0.0853, 0.0866,\n",
       "        0.0948, 0.0935, 0.0881, 0.0990, 0.0844, 0.0868, 0.0889, 0.0910, 0.0916,\n",
       "        0.0960, 0.0853, 0.0939, 0.0908, 0.0908])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[0,0,9,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9453e2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4663, 0.6051, 0.4732, 0.4689, 0.4473, 0.5631, 0.5896, 0.5327, 0.3713,\n",
       "        0.5081, 0.5653, 0.5160, 0.5343, 0.4732, 0.5112, 0.6187, 0.4419, 0.3951,\n",
       "        0.6316, 0.5591, 0.5036, 0.7060, 0.6905, 0.2582, 0.5555, 0.4821, 0.5335,\n",
       "        0.5393, 0.4463, 0.5635, 0.5828, 0.3450])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, H, N, D = 2, 4, 10, 32\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "q = torch.rand(B, H, N, D)\n",
    "k = torch.rand(B, H, N, D)\n",
    "v = torch.rand(B, H, N, D)\n",
    "# q[:,:,9,:] = 10.\n",
    "# k[:,:,9,:] = 10.\n",
    "# v[:,:,9,:] = 10.\n",
    "o = copy_model.causal_linear_attn_elu(q, k, v)\n",
    "o[0,0,9,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a422a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8491acc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763503dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
